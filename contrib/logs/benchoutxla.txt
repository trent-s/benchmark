
Trying pytest bench on model BERT_pytorch

============================= test session starts ==============================
platform linux -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /root/git/benchmarkxla
plugins: anyio-2.2.0, hypothesis-6.29.3, benchmark-4.0.0
collected 359 items / 357 deselected / 2 selected

../test_bench_xla.py FF                                                  [100%]

=================================== FAILURES ===================================
_______________ TestBenchNetwork.test_eval[BERT_pytorch-xla-jit] _______________
components._impl.workers.subprocess_rpc.ChildTraceException: Traceback (most recent call last):
  File "/root/git/benchmarkxla/components/_impl/workers/subprocess_rpc.py", line 482, in _run_block
    exec(  # noqa: P204
  File "<subprocess-worker>", line 35, in <module>
  File "<subprocess-worker>", line 12, in _run_in_worker_f
  File "/root/git/benchmarkxla/torchbenchmark/util/model.py", line 24, in __call__
    obj.__post__init__()
  File "/root/git/benchmarkxla/torchbenchmark/util/model.py", line 134, in __post__init__
    self.eager_output = stableness_check(self, cos_sim=False, deepcopy=self.DEEPCOPY, rounds=1)
  File "/root/git/benchmarkxla/torchbenchmark/util/env_check.py", line 78, in stableness_check
    previous_result = copy_model.invoke()
  File "/root/git/benchmarkxla/torchbenchmark/util/model.py", line 293, in invoke
    out = self.eval()
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/__init__.py", line 170, in eval
    next_sent_output, mask_lm_output = model.model.forward(*self.example_inputs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/language_model.py", line 24, in forward
    x = self.bert(x, segment_label)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/bert.py", line 43, in forward
    x = self.embedding(x, segment_info)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/embedding/bert.py", line 32, in forward
    x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_decomp/decompositions.py", line 1052, in embedding
    return weight[indices]
RuntimeError: /pytorch/xla/torch_xla/csrc/aten_xla_type.cpp:1395 : Check failed: bridge::IsXlaTensor(self) && indices_on_cpu_or_xla 
*** Begin stack trace ***
	tsl::CurrentStackTrace[abi:cxx11]()
	torch_xla::XLANativeFunctions::index(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
	
	
	
	
	at::_ops::index_Tensor::call(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
	torch::autograd::THPVariable_getitem(_object*, _object*)
	PyObject_GetItem
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	PyObject_Call
	
	at::functionalization::functionalize_op_helper(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
	c10::impl::BoxedKernelWrapper<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), void>::call(c10::BoxedKernel const&, c10::OperatorHandle const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	at::functionalization::_functionalize_aten_op<at::_ops::embedding, true, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	torch_xla::XLANativeFunctions::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	
	
	
	
	at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	
	at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	PyCFunction_Call
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	PyEval_EvalCode
	
	
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	PyEval_EvalCode
	
	
	
	PyRun_InteractiveLoopFlags
	PyRun_AnyFileExFlags
	
	Py_BytesMain
	__libc_start_main
	
*** End stack trace ***
indices should be either on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.

    working_dir: /tmp/tmp5wsvfsdc
    stdout:
        [2023-04-05] 07:12:05.677348: TIMER_SUBPROCESS_BEGIN_EXEC
        [2023-04-05] 07:12:10.064930: TIMER_SUBPROCESS_FAILED
        [2023-04-05] 07:12:10.065021: TIMER_SUBPROCESS_FINISHED
        [2023-04-05] 07:12:10.065041: TIMER_SUBPROCESS_BEGIN_READ

    stderr:


The above exception was the direct cause of the following exception:

self = <test_bench_xla.TestBenchNetwork object at 0x7f2446eb2ca0>
model_path = '/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch'
device = 'xla', compiler = 'jit'
benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7f2446e43490>
pytestconfig = <_pytest.config.Config object at 0x7f24605a0640>

    def test_eval(self, model_path, device, compiler, benchmark, pytestconfig):
        try:
            if skip_by_metadata(test="eval", device=device, jit=(compiler == 'jit'), \
                                extra_args=[], metadata=get_metadata_from_yaml(model_path)):
                raise NotImplementedError("Test skipped by its metadata.")
            # TODO: skipping quantized tests for now due to BC-breaking changes for prepare
            # api, enable after PyTorch 1.13 release
            if "quantized" in model_path:
                return
            task = ModelTask(model_path)
            if not task.model_details.exists:
                return  # Model is not supported.
    
>           task.make_model_instance(test="eval", device=device, jit=(compiler == 'jit'))

../test_bench_xla.py:98: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
../components/_impl/tasks/base.py:278: in inner
    self.worker.run(src)
../components/_impl/workers/subprocess_worker.py:155: in run
    self._run(snippet)
../components/_impl/workers/subprocess_worker.py:320: in _run
    subprocess_rpc.SerializedException.raise_from(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

serialized_e = SerializedException(_is_serializable=True, _type_bytes=b'\x80\x04\x95\x1d\x00\x00\x00\x00\x00\x00\x00\x8c\x08builtins\...on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.\n')
extra_context = '    working_dir: /tmp/tmp5wsvfsdc\n    stdout:\n        [2023-04-05] 07:12:05.677348: TIMER_SUBPROCESS_BEGIN_EXEC\n  ....065021: TIMER_SUBPROCESS_FINISHED\n        [2023-04-05] 07:12:10.065041: TIMER_SUBPROCESS_BEGIN_READ\n\n    stderr:\n'

    @staticmethod
    def raise_from(
        serialized_e: "SerializedException",
        extra_context: typing.Optional[str] = None,
    ) -> None:
        """Revive `serialized_e`, and raise.
    
        We raise the revived exception type (if possible) so that any higher
        try catch logic will see the original exception type. In other words:
        ```
            try:
                worker.run("assert False")
            except AssertionError:
                ...
        ```
    
        will flow identically to:
    
        ```
            try:
                assert False
            except AssertionError:
                ...
        ```
    
        If for some reason we can't move the true exception type to the main
        process (e.g. a custom Exception) we raise UnserializableException as
        a fallback.
        """
        if serialized_e._is_serializable:
            revived_type = ExceptionUnpickler.load_bytes(data=serialized_e._type_bytes)
            e = revived_type(*marshal.loads(serialized_e._args_bytes))
        else:
            e = UnserializableException(serialized_e._type_repr, serialized_e._args_repr)
    
        traceback_str = serialized_e._traceback_print
        if extra_context:
            traceback_str = f"{traceback_str}\n{extra_context}"
    
>       raise e from ChildTraceException(traceback_str)
E       RuntimeError: /pytorch/xla/torch_xla/csrc/aten_xla_type.cpp:1395 : Check failed: bridge::IsXlaTensor(self) && indices_on_cpu_or_xla 
E       *** Begin stack trace ***
E       	tsl::CurrentStackTrace[abi:cxx11]()
E       	torch_xla::XLANativeFunctions::index(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
E       	
E       	
E       	
E       	
E       	at::_ops::index_Tensor::call(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
E       	torch::autograd::THPVariable_getitem(_object*, _object*)
E       	PyObject_GetItem
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	PyObject_Call
E       	
E       	at::functionalization::functionalize_op_helper(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
E       	c10::impl::BoxedKernelWrapper<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), void>::call(c10::BoxedKernel const&, c10::OperatorHandle const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	at::functionalization::_functionalize_aten_op<at::_ops::embedding, true, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	torch_xla::XLANativeFunctions::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	
E       	
E       	
E       	
E       	at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	
E       	at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	PyCFunction_Call
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	PyEval_EvalCode
E       	
E       	
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	PyEval_EvalCode
E       	
E       	
E       	
E       	PyRun_InteractiveLoopFlags
E       	PyRun_AnyFileExFlags
E       	
E       	Py_BytesMain
E       	__libc_start_main
E       	
E       *** End stack trace ***
E       indices should be either on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.

../components/_impl/workers/subprocess_rpc.py:458: RuntimeError
______________ TestBenchNetwork.test_eval[BERT_pytorch-xla-eager] ______________
components._impl.workers.subprocess_rpc.ChildTraceException: Traceback (most recent call last):
  File "/root/git/benchmarkxla/components/_impl/workers/subprocess_rpc.py", line 482, in _run_block
    exec(  # noqa: P204
  File "<subprocess-worker>", line 2, in <module>
  File "/root/git/benchmarkxla/torchbenchmark/util/model.py", line 293, in invoke
    out = self.eval()
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/__init__.py", line 170, in eval
    next_sent_output, mask_lm_output = model.model.forward(*self.example_inputs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/language_model.py", line 24, in forward
    x = self.bert(x, segment_label)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/bert.py", line 43, in forward
    x = self.embedding(x, segment_info)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch/bert_pytorch/model/embedding/bert.py", line 32, in forward
    x = self.token(sequence) + self.position(sequence) + self.segment(segment_label)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/modules/sparse.py", line 162, in forward
    return F.embedding(
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/nn/functional.py", line 2210, in embedding
    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)
  File "/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/torch/_decomp/decompositions.py", line 1052, in embedding
    return weight[indices]
RuntimeError: /pytorch/xla/torch_xla/csrc/aten_xla_type.cpp:1395 : Check failed: bridge::IsXlaTensor(self) && indices_on_cpu_or_xla 
*** Begin stack trace ***
	tsl::CurrentStackTrace[abi:cxx11]()
	torch_xla::XLANativeFunctions::index(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
	
	
	
	
	at::_ops::index_Tensor::call(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
	torch::autograd::THPVariable_getitem(_object*, _object*)
	PyObject_GetItem
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	PyObject_Call
	
	at::functionalization::functionalize_op_helper(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
	c10::impl::BoxedKernelWrapper<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), void>::call(c10::BoxedKernel const&, c10::OperatorHandle const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	at::functionalization::_functionalize_aten_op<at::_ops::embedding, true, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	torch_xla::XLANativeFunctions::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	
	
	
	
	at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	
	at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
	
	
	PyCFunction_Call
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyEval_EvalCodeWithName
	_PyObject_Call_Prepend
	
	_PyObject_MakeTpCall
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	PyObject_Call
	_PyEval_EvalFrameDefault
	_PyFunction_Vectorcall
	
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	PyEval_EvalCode
	
	
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	_PyFunction_Vectorcall
	
	_PyEval_EvalCodeWithName
	PyEval_EvalCode
	
	
	
	PyRun_InteractiveLoopFlags
	PyRun_AnyFileExFlags
	
	Py_BytesMain
	__libc_start_main
	
*** End stack trace ***
indices should be either on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.

    working_dir: /tmp/tmpjpttlt8g
    stdout:
        [2023-04-05] 07:12:18.259236: TIMER_SUBPROCESS_BEGIN_EXEC
        [2023-04-05] 07:12:18.285444: TIMER_SUBPROCESS_FAILED
        [2023-04-05] 07:12:18.285506: TIMER_SUBPROCESS_FINISHED
        [2023-04-05] 07:12:18.285530: TIMER_SUBPROCESS_BEGIN_READ

    stderr:


The above exception was the direct cause of the following exception:

self = <test_bench_xla.TestBenchNetwork object at 0x7f2446c92be0>
model_path = '/root/git/benchmarkxla/torchbenchmark/models/BERT_pytorch'
device = 'xla', compiler = 'eager'
benchmark = <pytest_benchmark.fixture.BenchmarkFixture object at 0x7f2446c92f70>
pytestconfig = <_pytest.config.Config object at 0x7f24605a0640>

    def test_eval(self, model_path, device, compiler, benchmark, pytestconfig):
        try:
            if skip_by_metadata(test="eval", device=device, jit=(compiler == 'jit'), \
                                extra_args=[], metadata=get_metadata_from_yaml(model_path)):
                raise NotImplementedError("Test skipped by its metadata.")
            # TODO: skipping quantized tests for now due to BC-breaking changes for prepare
            # api, enable after PyTorch 1.13 release
            if "quantized" in model_path:
                return
            task = ModelTask(model_path)
            if not task.model_details.exists:
                return  # Model is not supported.
    
            task.make_model_instance(test="eval", device=device, jit=(compiler == 'jit'))
    
            with task.no_grad(disable_nograd=pytestconfig.getoption("disable_nograd")):
>               benchmark(task.invoke)

../test_bench_xla.py:101: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 
/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytest_benchmark/fixture.py:125: in __call__
    return self._raw(function_to_benchmark, *args, **kwargs)
/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytest_benchmark/fixture.py:147: in _raw
    duration, iterations, loops_range = self._calibrate_timer(runner)
/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytest_benchmark/fixture.py:275: in _calibrate_timer
    duration = runner(loops_range)
/root/anaconda3/envs/pytorch/lib/python3.8/site-packages/pytest_benchmark/fixture.py:90: in runner
    function_to_benchmark(*args, **kwargs)
../torchbenchmark/__init__.py:378: in invoke
    self.worker.run("""
../components/_impl/workers/subprocess_worker.py:155: in run
    self._run(snippet)
../components/_impl/workers/subprocess_worker.py:320: in _run
    subprocess_rpc.SerializedException.raise_from(
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

serialized_e = SerializedException(_is_serializable=True, _type_bytes=b'\x80\x04\x95\x1d\x00\x00\x00\x00\x00\x00\x00\x8c\x08builtins\...on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.\n')
extra_context = '    working_dir: /tmp/tmpjpttlt8g\n    stdout:\n        [2023-04-05] 07:12:18.259236: TIMER_SUBPROCESS_BEGIN_EXEC\n  ....285506: TIMER_SUBPROCESS_FINISHED\n        [2023-04-05] 07:12:18.285530: TIMER_SUBPROCESS_BEGIN_READ\n\n    stderr:\n'

    @staticmethod
    def raise_from(
        serialized_e: "SerializedException",
        extra_context: typing.Optional[str] = None,
    ) -> None:
        """Revive `serialized_e`, and raise.
    
        We raise the revived exception type (if possible) so that any higher
        try catch logic will see the original exception type. In other words:
        ```
            try:
                worker.run("assert False")
            except AssertionError:
                ...
        ```
    
        will flow identically to:
    
        ```
            try:
                assert False
            except AssertionError:
                ...
        ```
    
        If for some reason we can't move the true exception type to the main
        process (e.g. a custom Exception) we raise UnserializableException as
        a fallback.
        """
        if serialized_e._is_serializable:
            revived_type = ExceptionUnpickler.load_bytes(data=serialized_e._type_bytes)
            e = revived_type(*marshal.loads(serialized_e._args_bytes))
        else:
            e = UnserializableException(serialized_e._type_repr, serialized_e._args_repr)
    
        traceback_str = serialized_e._traceback_print
        if extra_context:
            traceback_str = f"{traceback_str}\n{extra_context}"
    
>       raise e from ChildTraceException(traceback_str)
E       RuntimeError: /pytorch/xla/torch_xla/csrc/aten_xla_type.cpp:1395 : Check failed: bridge::IsXlaTensor(self) && indices_on_cpu_or_xla 
E       *** Begin stack trace ***
E       	tsl::CurrentStackTrace[abi:cxx11]()
E       	torch_xla::XLANativeFunctions::index(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
E       	
E       	
E       	
E       	
E       	at::_ops::index_Tensor::call(at::Tensor const&, c10::List<c10::optional<at::Tensor> > const&)
E       	torch::autograd::THPVariable_getitem(_object*, _object*)
E       	PyObject_GetItem
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	PyObject_Call
E       	
E       	at::functionalization::functionalize_op_helper(c10::OperatorHandle const&, std::vector<c10::IValue, std::allocator<c10::IValue> >*)
E       	c10::impl::BoxedKernelWrapper<at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool), void>::call(c10::BoxedKernel const&, c10::OperatorHandle const&, c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	at::functionalization::_functionalize_aten_op<at::_ops::embedding, true, at::Tensor (at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)>::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	torch_xla::XLANativeFunctions::embedding_symint(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	
E       	
E       	
E       	
E       	at::_ops::embedding::redispatch(c10::DispatchKeySet, at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	
E       	at::_ops::embedding::call(at::Tensor const&, at::Tensor const&, c10::SymInt, bool, bool)
E       	
E       	
E       	PyCFunction_Call
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyEval_EvalCodeWithName
E       	_PyObject_Call_Prepend
E       	
E       	_PyObject_MakeTpCall
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	PyObject_Call
E       	_PyEval_EvalFrameDefault
E       	_PyFunction_Vectorcall
E       	
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	PyEval_EvalCode
E       	
E       	
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	_PyFunction_Vectorcall
E       	
E       	_PyEval_EvalCodeWithName
E       	PyEval_EvalCode
E       	
E       	
E       	
E       	PyRun_InteractiveLoopFlags
E       	PyRun_AnyFileExFlags
E       	
E       	Py_BytesMain
E       	__libc_start_main
E       	
E       *** End stack trace ***
E       indices should be either on cpu or on the same device as the indexed tensor (XLA). When using XLA, the indexed tensor must be an XLA tensor.

../components/_impl/workers/subprocess_rpc.py:458: RuntimeError
=============================== warnings summary ===============================
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
  /root/anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(modversion) < LooseVersion(min_module_version):

test_bench_xla.py::TestBenchNetwork::test_eval[BERT_pytorch-xla-jit]
  test_bench_xla.py:85: PytestBenchmarkWarning: Benchmark fixture was not used at all in this test!
    def test_eval(self, model_path, device, compiler, benchmark, pytestconfig):

-- Docs: https://docs.pytest.org/en/stable/warnings.html
=========================== short test summary info ============================
FAILED ../test_bench_xla.py::TestBenchNetwork::test_eval[BERT_pytorch-xla-jit]
FAILED ../test_bench_xla.py::TestBenchNetwork::test_eval[BERT_pytorch-xla-eager]
================ 2 failed, 357 deselected, 3 warnings in 14.95s ================


Trying pytest bench on model vgg16

============================= test session starts ==============================
platform linux -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /root/git/benchmarkxla
plugins: anyio-2.2.0, hypothesis-6.29.3, benchmark-4.0.0
collected 359 items / 357 deselected / 2 selected

../test_bench_xla.py ..                                                  [100%]

=============================== warnings summary ===============================
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
  /root/anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(modversion) < LooseVersion(min_module_version):

-- Docs: https://docs.pytest.org/en/stable/warnings.html

------------------------------------------------------------------------------------------- benchmark 'hub': 2 tests -------------------------------------------------------------------------------------------
Name (time in us)                     Min                   Max                  Mean              StdDev                Median                IQR            Outliers         OPS            Rounds  Iterations
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_eval[vgg16-xla-jit]         734.2400 (1.0)      1,947.1890 (1.0)        766.9340 (1.0)      111.5528 (1.0)        742.3139 (1.0)      14.4345 (1.0)         21;59  1,303.8932 (1.0)         513           1
test_eval[vgg16-xla-eager]     1,017.5358 (1.39)     5,029.5431 (2.58)     1,051.6421 (1.37)     133.6326 (1.20)     1,034.3280 (1.39)     37.8772 (2.62)         3;18    950.8938 (0.73)        939           1
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
========== 2 passed, 357 deselected, 2 warnings in 423.62s (0:07:03) ===========


Trying pytest bench on model resnet18

============================= test session starts ==============================
platform linux -- Python 3.8.8, pytest-6.2.3, py-1.10.0, pluggy-0.13.1
benchmark: 4.0.0 (defaults: timer=time.perf_counter disable_gc=False min_rounds=5 min_time=0.000005 max_time=1.0 calibration_precision=10 warmup=False warmup_iterations=100000)
rootdir: /root/git/benchmarkxla
plugins: anyio-2.2.0, hypothesis-6.29.3, benchmark-4.0.0
collected 359 items / 357 deselected / 2 selected

../test_bench_xla.py ..                                                  [100%]

=============================== warnings summary ===============================
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
../../../anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158
  /root/anaconda3/envs/pytorch/lib/python3.8/site-packages/sympy/external/importtools.py:158: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
    if LooseVersion(modversion) < LooseVersion(min_module_version):

-- Docs: https://docs.pytest.org/en/stable/warnings.html

--------------------------------------------------------------------------------------------- benchmark 'hub': 2 tests ---------------------------------------------------------------------------------------------
Name (time in us)                        Min                    Max                  Mean              StdDev                Median                IQR            Outliers         OPS            Rounds  Iterations
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
test_eval[resnet18-xla-jit]         872.8649 (1.0)      14,515.2670 (2.27)       962.1254 (1.0)      670.7067 (3.25)       913.6440 (1.0)      58.0552 (1.0)          7;20  1,039.3655 (1.0)         421           1
test_eval[resnet18-xla-eager]     2,317.0619 (2.65)      6,391.0780 (1.0)      2,402.9897 (2.50)     206.1519 (1.0)      2,366.6259 (2.59)     96.2181 (1.66)          2;2    416.1483 (0.40)        413           1
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------

Legend:
  Outliers: 1 Standard Deviation from Mean; 1.5 IQR (InterQuartile Range) from 1st Quartile and 3rd Quartile.
  OPS: Operations Per Second, computed as 1 / Mean
========== 2 passed, 357 deselected, 2 warnings in 107.32s (0:01:47) ===========

